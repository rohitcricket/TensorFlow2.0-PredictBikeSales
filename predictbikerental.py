# -*- coding: utf-8 -*-
"""PredictBikeRental.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zebOvhBc0hgldJgbULff2nI73-xeBw7w

Import Libraries
"""

!pip install tensorflow-gpu==2.0.0.alpha0

import tensorflow as tf
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

"""IMPORT DATASETS"""

from google.colab import drive
drive.mount('/content/drive')

bike = pd.read_csv('/content/drive/My Drive/Colab Notebooks/bike-sharing-daily.csv')

bike

bike.head(5)

bike.info()

bike.describe()

"""CLEAN UP DATASET"""

sns.heatmap(bike.isnull())

"""The above heatmap shows no missing data!"""

bike = bike.drop(labels = ['instant'], axis = 1)

bike

bike = bike.drop(labels = ['casual', 'registered'], axis = 1)

bike

bike.dteday = pd.to_datetime(bike.dteday, format = '%m/%d/%Y')

bike

bike.index = pd.DatetimeIndex(bike.dteday)

bike

bike = bike.drop(labels = ['dteday'], axis = 1)

bike

"""VISUALIZE DATASET"""

bike['cnt'].asfreq('W').plot(linewidth = 3)
plt.title('Bike Usage Per week')
plt.xlabel('Week')
plt.ylabel('Bike Rental')

bike['cnt'].asfreq('M').plot(linewidth = 3)
plt.title('Bike Usage Per Month')
plt.xlabel('Month')
plt.ylabel('Bike Rental')

bike['cnt'].asfreq('Q').plot(linewidth = 3)
plt.title('Bike Usage Per Quarter')
plt.xlabel('Quarter')
plt.ylabel('Bike Rental')

sns.pairplot(bike)

X_numerical = bike[['temp', 'hum', 'windspeed', 'cnt']]

X_numerical

sns.pairplot(X_numerical)

sns.heatmap(X_numerical.corr(), annot =True)

"""CREATE TRAINING AND TESTING DATASET"""

X_cat = bike[['season', 'yr', 'mnth', 'holiday', 'weekday', 'workingday', 'weathersit']]

X_cat

from sklearn.preprocessing import OneHotEncoder
onehotencoder = OneHotEncoder()
X_cat = onehotencoder.fit_transform(X_cat).toarray()

X_cat

X_cat.shape

X_cat = pd.DataFrame(X_cat)

X_numerical

X_numerical = X_numerical.reset_index()

X_all = pd.concat([X_cat, X_numerical], axis = 1)

X_all

X_all = X_all.drop('dteday', axis = 1)

X_all

X = X_all.iloc[:, :-1].values
y = X_all.iloc[:, -1:].values

X.shape

y.shape

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
y = scaler.fit_transform(y)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)

X_train.shape

X_test.shape

"""TRAIN THE MODEL"""

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(units=100, activation='relu', input_shape=(35, )))
model.add(tf.keras.layers.Dense(units=100, activation='relu'))
model.add(tf.keras.layers.Dense(units=100, activation='relu'))
model.add(tf.keras.layers.Dense(units=1, activation='linear'))

model.summary()

model.compile(optimizer='Adam', loss='mean_squared_error')

epochs_hist = model.fit(X_train, y_train, epochs = 25, batch_size = 50, validation_split = 0.2)

"""EVALUATE THE MODEL"""

epochs_hist.history.keys()

plt.plot(epochs_hist.history['loss'])
plt.plot(epochs_hist.history['val_loss'])
plt.title('Model Loss Progress During Training')
plt.xlabel('Epoch')
plt.ylabel('Training and Validation Loss')
plt.legend(['Training Loss', 'Validation Loss'])

y_predict = model.predict(X_test)
plt.plot(y_test, y_predict, "^", color = 'r')
plt.xlabel('Model Predictions')
plt.ylabel('True Values')

y_predict_orig = scaler.inverse_transform(y_predict)
y_test_orig = scaler.inverse_transform(y_test)

plt.plot(y_test_orig, y_predict_orig, "^", color = 'b')
plt.xlabel('Model Predictions')
plt.ylabel('True Values')

"""Find independent parameters"""

k = X_test.shape[1]

n = len(X_test)

n

k

from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from math import sqrt

RMSE = float(format(np.sqrt(mean_squared_error(y_test_orig, y_predict_orig)),'.3f'))
MSE = mean_squared_error(y_test_orig, y_predict_orig)
MAE = mean_absolute_error(y_test_orig, y_predict_orig)
r2 = r2_score(y_test_orig, y_predict_orig)
adj_r2 = 1-(1-r2)*(n-1)/(n-k-1)

print('RMSE =',RMSE, '\nMSE =',MSE, '\nMAE =',MAE, '\nR2 =', r2, '\nAdjusted R2 =', adj_r2)